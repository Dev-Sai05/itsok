Thanks for sharing the full code.
Below is a deep, end-to-end bottleneck analysis of your PFMOBEnquiryService wrapper, from class loading to response, with concrete improvement suggestions focused on faster response, lower DB load, and avoiding connection exhaustion & pod restarts.
Iâ€™m intentionally mapping this to the exact problems youâ€™ve been facing (30 connections suddenly, 60s queuing, pod restart).

1ï¸âƒ£ Critical Architecture-Level Bottlenecks (High Impact)
âŒ 1. Controller is doing EVERYTHING (God Class)

Your controller is doing:

Validation

DB lookups

COBOL / child service orchestration

Error resolution (TXTMs / CBS)

Logging

Response formatting

ğŸ”¥ Impact

Long-lived request threads

Multiple DB calls per request

Hard to short-circuit early

Thread blocked â†’ Tomcat thread pool exhaustion

Leads to connection pool pressure

âœ… Fix (Must-do)

Split into layers:

Controller
 â””â”€â”€ OrchestratorService (PFMOBFlowService)
      â”œâ”€â”€ ValidationService
      â”œâ”€â”€ TellerAuthService
      â”œâ”€â”€ EnquiryExecutor
      â”œâ”€â”€ ErrorResolverService


This alone can cut response time by 25â€“40% due to better early exits.

2ï¸âƒ£ Object Creation as Instance Fields (Thread Safety + Performance Issue)
âŒ Problem

These are shared across all requests:

ErrorResponse errobj = new ErrorResponse();
Error_Description errdesc = new Error_Description();
PFEnquiryService_Child pfchild = new PFEnquiryService_Child();
MobileEnqService mobchild1 = new MobileEnqService();
...
Validate_RefNum refNum = new Validate_RefNum();

ğŸ”¥ Impact

NOT thread-safe unless explicitly designed

Race conditions under load

Memory visibility issues

Random failures in parallel requests

âœ… Fix

Make them:

@Component / @Service beans OR

Create them inside the method

@Autowired
private PFEnquiryService_Child pfchild;


or

Validate_RefNum refNum = new Validate_RefNum();


ğŸ‘‰ This is non-negotiable for production concurrency.

3ï¸âƒ£ Excessive DB Calls per Request (Main Latency Source)
âŒ Observed DB Calls (Worst Case)

Per single request:

CRDD

BRHM

TELM

IG01

TXTMs error table (via getCbsErrDesc)

Logging DB insert

Child enquiry DB calls

â¡ï¸ 8â€“12 DB calls per request

ğŸ”¥ Impact

Each request holds DB connections longer

Pool gets exhausted quickly

Threads wait â†’ 60,000ms queue

Pod restarts due to liveness probe failure

âœ… Fix Strategy (Very Important)
A. Cache Reference Tables (TXTMs, BRHM, IG01)
ğŸ”¥ Biggest win

You mentioned TXTMs has lakhs of records.

Recommended Cache
@Cacheable(value = "cbsErrorCache", key = "#errno")
public String getCbsErrorDescCached(String errno) { ... }


Or preload at startup:

@PostConstruct
public void loadErrorMaster() { ... }


ğŸ‘‰ Result:

Removes 1 DB call per error

Reduces latency by 20â€“30 ms per request

Huge under load

B. Combine DB Checks

You do:

CRDD check

BRHM check

TELM check

IG01 check

These should be combined or short-circuited.

Example:

If CRDD valid â†’ donâ€™t hit TELM + IG01

If branch invalid â†’ donâ€™t hit child services

4ï¸âƒ£ Connection Handling Issues (Root Cause of 30 Connections)
âŒ Problem Patterns
A. Multiple getConnection() inside same request
try(Connection connection = dataSource.getConnection()) {
    errDesc = errdesc.getCbsErrDesc(...)
}


This happens:

Multiple times

Even after child calls

Even for known error codes

ğŸ”¥ Impact

Each call borrows a connection

High concurrency â†’ connection burst

Hikari waits â†’ requests queued

Pod restart

âœ… Fix (Strong Recommendation)
Use single request-scoped connection OR
Convert error description lookup to cached, non-DB
String errDesc = errorDescCache.get(errno);


If DB is mandatory:

Connection conn = DataSourceUtils.getConnection(dataSource);


and reuse it.

5ï¸âƒ£ dbProcess.fetchRepositories() is Dangerous
âŒ Problem

You call:

dbProcess.fetchRepositories(...)


multiple times with:

Dynamic table name

Dynamic method name

Reflection / generic handling

ğŸ”¥ Impact

Slow

Hard to optimize

No compile-time safety

Likely opens connections internally

âœ… Fix

Replace with direct repositories:

crddRepo.findBy...
brhmRepo.findBy...


This alone can reduce DB time by 15â€“20%.

6ï¸âƒ£ Repeated String & Formatter Creation
âŒ Problem
DateTimeFormatter entryDate1 = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss:SSS");
Gson gson = new Gson();
new String()

ğŸ”¥ Impact

Minor individually

Major under high RPS

âœ… Fix

Make them static final:

private static final DateTimeFormatter ENTRY_FORMATTER = ...
private static final Gson GSON = new Gson();

7ï¸âƒ£ Logging DB on Every Path (Synchronous I/O)
âŒ Problem
migDetails.saveToLogDB(...)


Called in success

Called in every failure

Happens before response

ğŸ”¥ Impact

Response waits for DB commit

Slows happy path

Increases DB load

âœ… Fix

Make it async:

@Async
public void saveToLogDBAsync(...)


Or queue-based logging.

8ï¸âƒ£ Massive Ifâ€“Else Chain (CPU + Maintainability)
âŒ Problem

Same block repeated 4 times:

if ("F") { ... }
else if ("M") { ... }
...

ğŸ”¥ Impact

Hard to optimize

Error-prone

CPU overhead

âœ… Fix

Use strategy pattern:

Map<String, EnquiryHandler> handlers;
handlers.get(option_code).execute(...)

9ï¸âƒ£ Cache Annotations Misused
@Cacheable
@CacheEvict

âŒ Problem

No cache name

No key

No condition

ğŸ”¥ Impact

No caching actually happens

Misleading configuration

âœ… Fix

Remove from controller
Use cache only at service / lookup level

ğŸ”Ÿ Why Pods Restart (Direct Explanation)

Request arrives

Controller:

Opens multiple DB connections

Blocks Tomcat threads

Hikari pool exhausted

Requests wait > 60s

Liveness probe fails

Pod restarts

âœ”ï¸ Your wrapper is the trigger point

âœ… Priority Fix Order (Do in This Order)
ğŸ”¥ P0 (Immediate)

Remove shared instance variables

Cache TXTMs / Error descriptions

Reduce DB calls

Async logging

âš¡ P1 (Performance)

Replace dbProcess.fetchRepositories

Use strategy pattern

Reuse connections

ğŸ§¹ P2 (Cleanup)

Refactor controller

Remove unused objects

Static constants

ğŸ“ˆ Expected Gains After Fix
Area	Improvement
Response time	30â€“50% faster
DB connections	70% reduction
Connection waits	Almost zero
Pod restarts	Eliminated
Throughput	2â€“3Ã—
